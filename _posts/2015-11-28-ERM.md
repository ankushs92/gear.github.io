---
layout: post
type: math
title: Empirical Risk Minimization
subtitle: Some intuitions and problems
---

I remember the confusion I had when professor [Osamu Watanabe](http://www.is.titech.ac.jp/~watanabe/)
first gave me the [Occam's razor problem](https://cdn.rawgit.com/gear/Assignments/master/fmcs_a1.pdf). 
With that confusion, I start to love the exciting world of theoretical machine learning. This post is 
about ERM scheme and some proofs around it. This post is my understanding of the second chapter
in the book **Under standing machine learning: From theory of algorithm** by Shai Shalev-Shwartz and 
Shai Ben-David.

## Playground

Here are some notations and scheme for our playground:

- Data space: $$\mathcal{X}$$ - Where we "draw" data samples.
- Data distribution: $$\mathcal{D}$$ - How the data in $$\mathcal{X}$$ is distributed.
- Label space: $$\mathcal{Y}$$ - Where we have the labels for our samples.
- Labeling function: $$f: \mathcal{X} \mapsto \mathcal{Y}$$ - Given a data sample, return its true label.
- Training set: $$S$$ of size $$size(S) = m$$ - The set of training samples and its training labels.
- Hypotheses class: $$\mathcal{H}$$ - A finite set of hypotheses $$h$$. 
- Hypothesis: $$h$$ - An estimation of $$f$$, $$h$$ is the result of some learning algorithm and $$S$$. 
Also called "prediction rule" in this post.
- **Our task**: Given a training set $$S$$, we do not know $$D$$ or $$f$$ for all data points, how do
we argue about the _quality_ of the learned hypothesis $$h$$ compared to $$f$$?

## Measuring risk on the training dataset

Suppose you are a hero going on an adventure, then suddenly the Orange Monster
appears, gives you a bunch of... emails and challenges you to mark which email
is an important email. The Orange Monster knows the **true** labeling of each
email it gave you, it just wants to test your wisdom. Let's say you invented
some way (let's call that way $$h$$ - stands for _hypothesis_) to mark the 
emails. How do you or the Orange Monster evaluate your solution? Obviously 
(or empirically), you would take the number of _incorrect_ solution, divide
it to the total number of emails ($$m$$). And that is how you compute the
Empirical Risk. Machine learning works in a similar way, if you give it a
bunch of emails and ask it to classify which email is spam email, the machine
will need some kind of evaluation to measure its own prediction. However,
both we and the machines do not know about the true distribution of emails.
Unfortunately we can only evaluate our predictions on the data given to us.
Our journey begins when we think: "Hmm, so how much the empirical risk can
tell us about the performance of our prediction algorithm on the real data
population?".

![ERM Robots]({{site.baseurl}}/img/erm_mailbot.png)

Empirical Risk Minimization (ERM) is a class of algorithm that returns the hypothesis $$h$$
that minimizes $$L_S(h)$$ given a set of training samples $$S$$. The empirical risk is 
defined as:

$$ L_S(h) = \frac{|\{i \in [m]: h(x_i) \neq f(x_i)\}|}{m} $$

This formula can be read as: The _empirical risk_ is the number of samples ($$x_i$$) at which our 
predictor $$h$$ doesn't match with the true labeling function $$f$$ (subscript S denotes the risk is based
on the training set $$S$$ of size $$m$$ only). In this scheme, we are given a set of hypotheses
$$\mathcal{H}$$, a data space $$\mathcal{X}$$, a distribution $$\mathcal{D}$$ over the data space
$$\mathcal{X}$$, and a labeling function $$f: \mathcal{X} \mapsto \mathcal{Y}$$ that labels a
data samples to its "true" label. **An algorithm is called ERM when it returns the hypothesis
that minimizes the empirical risk**. In another word, empirical risk can be viewed as the loss
over the training data $$S$$. It is possible that  The weakness of _empirical risk_ 
(or empirical loss) is when the
training samples set $$S$$ contains only _misleading samples_. 

![Unlucky samples]({{site.baseurl}}/img/erm_misleading.png)

Idealistically, we want to know the **error of a prediction rule** $$L_{D,f}(h)$$. This term
can be interpreted as the error of predictor $$h$$ over (evaluated on) the data distribution $$D$$ 
and the labeling function $$f$$ (the true loss!). However, the data distribution $$D$$ and the true 
labeling function $$f$$ typically are not available in practice. Therefore, we have to rely on the 
empirical risk, which is evaluated on the training data $$S$$. By definition, the (true) **error
of a prediction rule** is:

$$ L_{D,f}(h) = \underset{x \sim \mathcal{D}} P [h(x) \neq f(x)] = \mathcal{D}(\{x : h(x) \neq f(x)\}) $$

The error of a prediction rule is the probability that the prediction rule returns a wrong label, 
given a data sample $$x_i$$ from $$\mathcal{D}$$.

## Overfitting in ERM

Since the risk (loss) is measured on the training data $$S$$, ERM is prone to over
fitting. Simply speaking, the output hypothesis $$h$$ from ERM can just map training
samples to training labels. If such hypothesis is chosen, the risk empirical $$L_S(h)$$ will 
be 0, but the prediction rule error is not guaranteed to be "small". In other words, we do 
not know if we have a "good" hypothesis or just an overfitting hypothesis over $$S$$. 
It is proven that the infinite polynomial threshold-based hypothesis class can always
overfit the training data.

## Bound for "bad" training samples

To address the case where the empirical risk is 0 but the hypothesis is "bad", let's call the
minimum acceptable _true_ risk $$L_{D,f}(h)$$ epsilon ($$\epsilon$$), and have two 
assumption about the data distribution and the hypothesis class:

- Each data sample in $$S$$ is distributed independently and identically (i.i.d.).
- There exists $$h^* \in \mathcal{H}$$ s.t. $$L_{D,f}(h^*) = 0$$ (Realizability Assumption).

We care (or worry) about the case where $$L_S(h) = 0$$, but $$L_{D,f}(h) > \epsilon$$, 
which means even the predictor (hypothesis) that yeilds the smallest empirical loss is 
still not good enough for the whole data prediciton.

$$ L_S(h_{bad}) = L_S(h^*) = 0, \mbox{but} $$

$$ L_{D,f}(h) > \epsilon \ge L_{D,f}(h^*) $$

Let's call the class of bad hypotheses $$\mathcal{H}_{bad}$$, which contains the hypotheses
that has the error prediction rule ($$L_{D,f}(h)$$) larger than $$\epsilon$$ for some
defined $$\epsilon$$. Now, we would like to know **how likely an ERM algorithm will
choose one of the bad hypotheses**. Formally, we would like to bound the probability
of getting the sample set $$S|_x$$ that leads to a bad hypothesis 
$$h_S \in \mathcal{H}_{bad}$$:

$$ \mathcal{D}^m ( \{ S|_x : L_{D,f}(h_S) > \epsilon \} ) $$

$$ \mathcal{H}_{bad} = \{ h \in \mathcal{H} : L_{D,f}(h) > \epsilon \} $$

In here, we analyze the effect of the sample size $$m$$ to the probability of 
getting a bad hypothesis from the ERM learning scheme. To be clear, in here,
**bad hypothesis** means the hypothesis that gives high error rate; **bad training
dataset** means the training dataset that makes ERM returns a bad hypothesis, and
**misleading training dataset** means the training dataset that have zero empirical
risk. The quick outline of the development for the upper bound is as follow:

![Bad Samples lead to bad hypothesis]({{site.baseurl}}/img/emr_bad.png)

<!--
TODO: Add image describe the bad samples and bad hypothesis scheme here
-->

- Prove that the sets of bad training datasets is a subset of set of all 
misleading training datasets.
- Expand set of all misleading training datasets by union of bad training 
datasets corresponding to each bad hypothesis.
- The probability of getting a bad training dataset (hence bad hypothesis)
is bounded by the probability of getting a misleading training dataset.
- Bound the probability of getting a misleading training dataset by union bound.
- Fix a misleading training set and a distribution, use i.i.d. assumption to bound 
the probability of getting that set. Use $$ (1-x) \le e^{-x} $$.
- Finally show that:

$$ \mathcal{D}^m(\{S|_x : L_{D,f}(h_S) > \epsilon \}) \le \| \mathcal{H}_{bad} \|  e^{-\epsilon m} $$ 

We call the set of _missleading_ examples $$M$$. This set contains sets of training
dataset $$S$$ that can lead to a bad hypothesis:

$$ M = \{S|_x : \exists h \in \mathcal{H}_{bad}, L_S(h) = 0 \}$$

TODO: ADD EX
<!--
## Here is a secondary heading

Here's a useless table:
 
| Number | Next number | Previous number |
| :------ |:--- | :--- |
| Five | Six | Four |
| Ten | Eleven | Nine |
| Seven | Eight | Six |
| Two | Three | One |
 

How about a yummy crepe?

![Crepe](http://s3-media3.fl.yelpcdn.com/bphoto/cQ1Yoa75m2yUFFbY2xwuqw/348s.jpg)

Here's a code chunk:

~~~
var foo = function(x) {
  return(x + 5);
}
foo(3)
~~~

And here is the same code with syntax highlighting:

```javascript
var foo = function(x) {
  return(x + 5);
}
foo(3)
```

And here is the same code yet again but with line numbers:

{% highlight javascript linenos %}
var foo = function(x) {
  return(x + 5);
}
foo(3)
{% endhighlight %}
-->
