---
layout: post
type: math
title: Empirical Risk Minimization
subtitle: Some intuitions and problems
---

I remember the confusion I had when professor [Osamu Watanabe](http://www.is.titech.ac.jp/~watanabe/)
first gave me the Occam's razor problem. With that confusion, I start to love the
exciting world of theoretical machine learning. This post is about ERM scheme and 
some proofs around it.

## Playground

Here are some notations and scheme for our playground:

- Data space: $$\mathcal{X}$$ - Where we "draw" data samples.
- Data distribution: $$\mathcal{D}$$ - How the data in $$\mathcal{X}$$ is distributed.
- Label space: $$\mathcal{Y}$$ - Where we have the labels for our samples.
- Labeling function: $$f: \mathcal{X} \mapsto \mathcal{Y}$$ - Given a data sample, return its true label.
- Training set: $$S$$ of size $$\|S\| = m$$ - The set of training samples and its training labels.
- Hypotheses class: $$\mathcal{H}$$ - A finite set of hypotheses $$h$$. 
- Hypothesis: $$h$$ - An estimation of $$f$$, $$h$$ is the result of some learning algorithm and $$S$$. Also called "prediction rule" in this post.
- **Our task**: Given a training set $$S$$, we do not know $$D$$ or $$f$$ for all data points, how do
we argue about the _quality_ of the learned hypothesis $$h$$ compared to $$f$$?

## Measuring risk on the training dataset

Empirical Rish Minimization (ERM) is a class of algorithm that returns the hypothesis $$h$$
that minimizes $$L_S(h)$$ given a set of training samples $$S$$. The empirical risk is 
defined as:

$$ L_S(h) = \frac{|\{i \in [m]: h(x_i) \neq f(x_i)\}|}{m} $$

This formula can be read as: The _empirical risk_ is the number of samples ($$x_i$$) at which our 
predictor $$h$$ doesn't match with the true labeling function $$f$$ (subscript S denotes the risk is based
on the training set $$S$$ of size $$m$$ only). In this scheme, we are given a set of hypotheses
$$\mathcal{H}$$, a data space $$\mathcal{X}$$, a distribution $$\mathcal{D}$$ over the data space
$$\mathcal{X}$$, and a labeling function $$f: \mathcal{X} \mapsto \mathcal{Y}$$ that labels a
data samples to its "true" label. **An algorithm is called ERM when it returns the hypothesis
that minimizes the empirical risk**. In another word, empirical risk can be viewed as the loss
over the training data $$S$$. The weakness of _empirical risk_ (or empirical loss) is when the
training samples set $$S$$ contains only _misleading samples_. 

Idealistically, we want to know the **error of a prediction rule** $$L_{D,f}(h)$$. This term
can be interpreted as the error of predictor $$h$$ over (evaluated on) the data distribution $$D$$ 
and the labeling function $$f$$ (the true loss!). However, the data distribution $$D$$ and the true 
labeling function $$f$$ typically are not available in practice. Therefore, we have to rely on the 
empirical risk, which is evaluated on the training data $$S$$. By definition, the (true) **error
of a prediction rule** is:

$$ L_{D,f}(h) = \underset{x \sim \mathcal{D}} P [h(x) \neq f(x)] = \mathcal{D}(\{x : h(x) \neq f(x)\}) $$

The error of a prediction rule is the probability that the prediction rule returns a wrong label, 
given a data sample $$x_i$$ from $$\mathcal{D}$$.

## Overfitting in ERM

Since the risk (loss) is measured on the training data $$S$$, ERM is prone to over
fitting. Simply speaking, the output hypothesis $$h$$ from ERM can just map training
samples to training labels. If such hypothesis is chosen, the risk empirical $$L_S(h)$$ will 
be 0, but the prediction rule error is not guaranteed to be "small". In other words, we do 
not know if we have a "good" hypothesis or just an overfitting hypothesis over $$S$$. 
It is proven that the infinite polynomial threshold-based hypothesis class can always
overfit the training data.

## Bound for "bad" training samples

To address the case where the empirical risk is 0 but the hypothesis is "bad", let's call the
minimum acceptable _true_ risk $$L_{D,f}(h)$$ epsilon ($$\epsilon$$), and have two 
assumption about the data distribution and the hypothesis class:

- Each data sample in $$S$$ is distributed independently and identically (i.i.d.).
- There exists $$h^* \in \mathcal{H}$$ s.t. $$L_{D,f}(h^*) = 0$$ (Realizability Assumption).

We care (or worry) about the case where $$L_S(h) = 0$$, but $$L_{D,f}(h) > \epsilon$$, 
which means even the predictor (hypothesis) that yeilds the smallest empirical loss is 
still not good enough for the whole data prediciton.

$$ L_S(h_{bad}) = L_S(h^*) = 0, \mbox{but} $$

$$ L_{D,f}(h) > \epsilon \ge L_{D,f}(h^*) $$

Let's call the class of bad hypotheses $$\mathcal{H}_{bad}$$, which contains the hypotheses
that has the error prediction rule ($$L_{D,f}(h)$$) larger than $$\epsilon$$ for some
defined $$\epsilon$$. Now, we would like to know **how likely an ERM algorithm will
choose one of the bad hypotheses**. Formally, we would like to bound the probability
of getting the sample set $$S|_x$$ that leads to a bad hypothesis 
$$h_S \in \mathcal{H}_{bad}$$:

$$ \mathcal{D}^m ( \{ S|_x : L_{D,f}(h_S) > \epsilon \} ) $$

$$ \mathcal{H}_{bad} = \{ h \in \mathcal{H} : L_{D,f}(h) > \epsilon \} $$

In this scheme, we analyze the effect of the sample size $$m$$ to the probability of 
getting a bad hypothesis from the ERM learning scheme. To be clear, in here,
**bad hypothesis** means the hypothesis that gives high error rate; **bad training
dataset** means the training dataset that makes ERM returns a bad hypothesis, and
**misleading training dataset** means the training dataset that have zero empirical
risk. The quick outline of the development for the upper bound is as follow:

<!--
TODO: Add image describe the bad samples and bad hypothesis scheme here
-->

- Prove that the sets of bad training datasets is a subset of set of all 
misleading training datasets.
- Expand set of all misleading training datasets by union of bad training 
datasets corresponding to each bad hypothesis.
- The probability of getting a bad training dataset (hence bad hypothesis)
is bounded by the probability of getting a misleading training dataset.
- Bound the probability of getting a misleading training dataset by union bound.
- Fix a misleading training set and a distribution, use i.i.d. assumption to bound 
the probability of getting that set. Use $$ (1-x) \le e^{-x} $$.
- Finally show that:

$$ \mathcal{D}^m(\{S|_x : L_{D,f}(h_S) > \epsilon \}) \le \| \mathcal{H}_{bad} \|  e^{-\epsilon m} $$ 

We call the set of _missleading_ examples $$M$$. This set contains sets of training
dataset $$S$$ that can lead to a bad hypothesis:

$$ M = \{S|_x : \exists h \in \mathcal{H}_{bad}, L_S(h) = 0 \}$$

<!--
## Here is a secondary heading

Here's a useless table:
 
| Number | Next number | Previous number |
| :------ |:--- | :--- |
| Five | Six | Four |
| Ten | Eleven | Nine |
| Seven | Eight | Six |
| Two | Three | One |
 

How about a yummy crepe?

![Crepe](http://s3-media3.fl.yelpcdn.com/bphoto/cQ1Yoa75m2yUFFbY2xwuqw/348s.jpg)

Here's a code chunk:

~~~
var foo = function(x) {
  return(x + 5);
}
foo(3)
~~~

And here is the same code with syntax highlighting:

```javascript
var foo = function(x) {
  return(x + 5);
}
foo(3)
```

And here is the same code yet again but with line numbers:

{% highlight javascript linenos %}
var foo = function(x) {
  return(x + 5);
}
foo(3)
{% endhighlight %}
-->
