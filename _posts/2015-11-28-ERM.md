---
layout: post
type: math
title: Empirical Rish Minimization
subtitle: A gentle approach to theoretical machine learning
---

I remember the confusion I had when professor [Osamu Watanabe](http://www.is.titech.ac.jp/~watanabe/)
first gave me the Occam's razor problem. With that confusion, I start to love the
exciting world of theoretical machine learning. In this post, I will explain the basic
of theoretical machine learning starting with ERM. 

Empirical Rish Minimization (ERM) is a class of algorithm that returns the hypothesis $$h$$
that minimizes $$L_S(h)$$ given a set of training samples $$S$$. The empirical risk is 
defined as:

$$ L_S(h) = \frac{|\{i \in [m]: h(x_i) \neq f(x_i)\}|}{m} $$

This formula can be read as: The _empirical risk_ is the number of samples ($$x_i$$) at which our 
predictor $$h$$ doesn't match with the true labeling function $$f$$ (subscript S denotes the risk is based
on the training set $$S$$ of size $$m$$ only). In this scheme, we are given a set of hypotheses
$$\mathcal{H}$$, a data space $$\mathcal{X}$$, a distribution $$\mathcal{D}$$ over the data space
$$\mathcal{X}$$, and a labeling function $$f: \mathcal{X} \mapsto \mathcal{Y}$$ that labels a
data samples to its "true" label. **An algorithm is called ERM when it returns the hypothesis
that minimizes the empirical risk**. In another word, empirical risk can be viewed as the loss
over the training data $$S$$. The weakness of _empirical risk_ (or empirical loss) is when the
training samples set $$S$$ contains only _misleading samples_. 

Idealistically, we want to know the **error of a prediction rule** $$L_{D,f}(h)$$. This term
can be interpreted as the error of predictor $$h$$ over (evaluated on) the data distribution $$D$$ 
and the labeling function $$f$$. The true loss! However, the data distribution $$D$$ and the true 
labeling function $$f$$ typically are not available in practice. Therefore, we have to rely on the 
empirical risk, which is evaluated on the training data $$S$$. By definition, the (true) **error
of a prediction rule** is:

$$ L_{D,f}(h) = \underset{x \sim \mathcal{D}} \mathds{P} [h(x) \neq f(x)] = \mathcal{D}({x : h(x) \neq f(x)}) $$

You can write regular [markdown](http://markdowntutorial.com/) here and Jekyll will automatically convert it to a nice webpage.  I strongly encourage you to [take 5 minutes to learn how to write in markdown](http://markdowntutorial.com/) - it'll teach you how to transform regular text into bold/italics/headings/tables/etc.

**Here is some bold text**

## Here is a secondary heading

Here's a useless table:
 
| Number | Next number | Previous number |
| :------ |:--- | :--- |
| Five | Six | Four |
| Ten | Eleven | Nine |
| Seven | Eight | Six |
| Two | Three | One |
 

How about a yummy crepe?

![Crepe](http://s3-media3.fl.yelpcdn.com/bphoto/cQ1Yoa75m2yUFFbY2xwuqw/348s.jpg)

Here's a code chunk:

~~~
var foo = function(x) {
  return(x + 5);
}
foo(3)
~~~

And here is the same code with syntax highlighting:

```javascript
var foo = function(x) {
  return(x + 5);
}
foo(3)
```

And here is the same code yet again but with line numbers:

{% highlight javascript linenos %}
var foo = function(x) {
  return(x + 5);
}
foo(3)
{% endhighlight %}
