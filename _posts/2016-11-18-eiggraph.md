---
layout: post
type: math
title: Eigenvalues of graphs
subtitle: The hidden world by eigenvalues
---

In engineering, it is always useful to know about the eigenvalues of a matrix. The
eigenvalues and their corresponding eigenvectors can tell us many properties of
the graphs. This post is the collection of my knowledge about eigenvalues.

## Linear algebra

Eigenvalue is defined only for square matrices (non-square matrices we have singular
values). Consider an $$n \times n$$ real matrix $$A$$. We can interpret matrix $$A$$ 
as a vector operator. I like to think of matrix multiplication with a real vector
as the act of rotating and scaling the vector. Out of all n-dimension vector, there
is a few that is special to a given matrix $$A$$. The vectors, when multiplied to 
the matrix $$A$$, is only scaled, not rotated (this way of thinking about the matrix
is particularly useful in engineering, which I learned from professor Gilbert Strang's
OCW lectures). Formally, if $$x$$ is an eigenvector of $$A$$:

$$ Ax = \lambda x, \mbox{for some real or complex} \lambda $$

In the formula above, $$x$$ is the eigenvector and $$\lambda$$ is the _corresponding_
eigenvalue. Interestingly, the trace (sum of diagonal elements) of matrix $$A$$ is 
the sum of all eigenvalues. 

- Data space: $$\mathcal{X}$$ - Where we "draw" data samples.
- Data distribution: $$\mathcal{D}$$ - How the data in $$\mathcal{X}$$ is distributed.
- Label space: $$\mathcal{Y}$$ - Where we have the labels for our samples.
- Labeling function: $$f: \mathcal{X} \mapsto \mathcal{Y}$$ - Given a data sample, return its true label.
- Training set: $$S$$ of size $$\|S\| = m$$ - The set of training samples and its training labels.
- Hypotheses class: $$\mathcal{H}$$ - A finite set of hypotheses $$h$$. 
- Hypothesis: $$h$$ - An estimation of $$f$$, $$h$$ is the result of some learning algorithm and $$S$$. Also called "prediction rule" in this post.
- **Our task**: Given a training set $$S$$, we do not know $$D$$ or $$f$$ for all data points, how do
we argue about the _quality_ of the learned hypothesis $$h$$ compared to $$f$$?

## Measuring risk on training dataset

Empirical Rish Minimization (ERM) is a class of algorithm that returns the hypothesis $$h$$
that minimizes $$L_S(h)$$ given a set of training samples $$S$$. The empirical risk is 
defined as:

$$ L_S(h) = \frac{|\{i \in [m]: h(x_i) \neq f(x_i)\}|}{m} $$

This formula can be read as: The _empirical risk_ is the number of samples ($$x_i$$) at which our 
predictor $$h$$ doesn't match with the true labeling function $$f$$ (subscript S denotes the risk is based
on the training set $$S$$ of size $$m$$ only). In this scheme, we are given a set of hypotheses
$$\mathcal{H}$$, a data space $$\mathcal{X}$$, a distribution $$\mathcal{D}$$ over the data space
$$\mathcal{X}$$, and a labeling function $$f: \mathcal{X} \mapsto \mathcal{Y}$$ that labels a
data samples to its "true" label. **An algorithm is called ERM when it returns the hypothesis
that minimizes the empirical risk**. In another word, empirical risk can be viewed as the loss
over the training data $$S$$. The weakness of _empirical risk_ (or empirical loss) is when the
training samples set $$S$$ contains only _misleading samples_. 

Idealistically, we want to know the **error of a prediction rule** $$L_{D,f}(h)$$. This term
can be interpreted as the error of predictor $$h$$ over (evaluated on) the data distribution $$D$$ 
and the labeling function $$f$$ (the true loss!). However, the data distribution $$D$$ and the true 
labeling function $$f$$ typically are not available in practice. Therefore, we have to rely on the 
empirical risk, which is evaluated on the training data $$S$$. By definition, the (true) **error
of a prediction rule** is:

$$ L_{D,f}(h) = \underset{x \sim \mathcal{D}} P [h(x) \neq f(x)] = \mathcal{D}(\{x : h(x) \neq f(x)\}) $$

The error of a prediction rule is the probability that the prediction rule returns a wrong label, 
given a data sample $$x_i$$ from $$\mathcal{D}$$.

## Overfitting in ERM

Since the risk (loss) is measured on the training data $$S$$, ERM is prone to over
fitting. Simply speaking, the output hypothesis $$h$$ from ERM can just map training
samples to training labels. If such hypothesis is chosen, the risk empirical $$L_S(h)$$ will 
be 0, but the prediction rule error is not guaranteed to be "small". In other words, we do 
not know if we have a "good" hypothesis or just an overfitting hypothesis over $$S$$. 
It is proven that the infinite polynomial threshold-based hypothesis class can always
overfit the training data.

## Bound for "bad" training samples

To address the case where the empirical risk is 0 but the hypothesis is "bad", let's call the
minimum acceptable _true_ risk $$L_{D,f}(h)$$ epsilon ($$\epsilon$$), and have two 
assumption about the data distribution and the hypothesis class:

- Each data sample in $$S$$ is distributed independently and identically (i.i.d.).
- There exists $$h^* \in \mathcal{H}$$ s.t. $$L_{D,f}(h^*) = 0$$ (Realizability Assumption).

We care (or worry) about the case where $$L_S(h) = 0$$, but $$L_{D,f}(h) > \epsilon$$, 
which means even the predictor (hypothesis) that yeilds the smallest empirical loss is 
still not good enough for the whole data prediciton (let's call such predictor "bad" 
predictor). Under the aforementioned assumptions, we can say that the reason for ERM to
yield "bad" predictor is "bad" training data. In other word, using empirical risk on
a "bad" training data, we cannot distinguish between a "bad" hypothesis and a good one.

$$ L_S(h_{\mbox{bad}}) = L_S(h^*) = 0, \mbox{but} $$

$$ L_{D,f}(h) > \epsilon \ge L_{D,f}(h^*) $$

## References

[1]: http://www.cs.elte.hu/~lovasz/eigenvals-x.pdf

<!--
## Here is a secondary heading

Here's a useless table:
 
| Number | Next number | Previous number |
| :------ |:--- | :--- |
| Five | Six | Four |
| Ten | Eleven | Nine |
| Seven | Eight | Six |
| Two | Three | One |
 

How about a yummy crepe?

![Crepe](http://s3-media3.fl.yelpcdn.com/bphoto/cQ1Yoa75m2yUFFbY2xwuqw/348s.jpg)

Here's a code chunk:

~~~
var foo = function(x) {
  return(x + 5);
}
foo(3)
~~~

And here is the same code with syntax highlighting:

```javascript
var foo = function(x) {
  return(x + 5);
}
foo(3)
```

And here is the same code yet again but with line numbers:

{% highlight javascript linenos %}
var foo = function(x) {
  return(x + 5);
}
foo(3)
{% endhighlight %}
-->
