---
layout: post
type: project
title: Motif-Aware Graph Embedding
subtitle: The benefit of guilding random walks by motifs
---

![It's like this]({{site.baseurl}}/img/mage_example.png)

As an electronics engineering major, I initially thought of "embedding" as a kind of small
sub-systems used as a small computer. Later, graph theory gives me
another view of "embedding":

> Graph embedding of a graph $$G$$ on a surface $$\Sigma$$ is a representation
of $$G$$ on $$\Sigma$$ in which points of $$\Sigma$$ are associated to vertices
and simple arcs are associated to edges such that connectivity is preserved and
there is no over-lapping between edges.

This view of "embedding" is more abtract, and somewhat more fun. I understand it
as the act of "drawing" a graph onto another surface (or space). It is well known
that any [planar graph](https://en.wikipedia.org/wiki/Planar_graph) can be embedded 
on a 2D surface, and any graph can be embedded on a 3D "surface". However, recently,
the word "embedding" seems to mean something more concrete to a machine learning practitioner:

> Embedding, as in "word embedding", refers to a set of modeling and feature learning
techniques that map structured tokens to vectors of real numbers.

I think embedding **is** dimensionality reduction or representation learning, 
just another different fancy name. Maybe, in the context of natural language processing
and graph processing, we call dimensionality reduction embedding (*＾∀ﾟ)b.

### Just a little history

There were many [representation learning](https://arxiv.org/abs/1206.5538) 
algorithms in the field of machine learning. One of the most famous method is 
[Spectral Clustering](http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf) 
in which the _similarity_ of data is preserved while the dimensionality is reduced. 
More recently, the [t-SNE](https://lvdmaaten.github.io/tsne/) algorithm is widely used
for data visualization. The general objective of t-SNE is somewhat similar to
Spectral Clustering. However, instead of dealing with large matrix computation, 
the authors solved the embedding problem by minimizing a loss function defined
for the representations of each data point in two spaces (original data space and
the embedding space). The result of t-SNE is a 2D or 3D representation of
high dimensional data.

Similar to the t-SNE technique, [DeepWalk](https://arxiv.org/abs/1403.6652) was
proposed by Perozzi et al at KDD'14. DeepWalk and other algorithms inspired by
it were based on the [Skipgram](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) NPL model proposed by Mikolov et al in 2013.
By observing the similarity between the vertex frequencie distribution on random walks
in a graph and the word frequency distribution in text, Perozzi et al proposed
that we can use random walks on a graph to generate "sentences", and then learn
the node representation using the Skipgram model. I wrote a post further explaining
Skipgram and DeepWalk [here]({{site.baseurl}}/2016-01-27-word2vec/). The slide
for my talk introducing DeepWalk at The University of Tokyo can be found [here](https://cdn.rawgit.com/gear/Presentations/master/2016-deepwalk-Todai/DeepWalk_2016_UTokyo.pdf).

We can make analogues for almost everything, and therefore analogues is... useless
(as said by RPF in his book "Surely You're Joking, Mr. Feynman"). However,
I would like to end this section with a little analogue which I prefer
the word "duality":

| | Text | Graph |
| :--- | :--- | :--- |
| Token | Word | Vertex |
| Structure | Sentences | Connections |
| Distribution | Power law (frequency) | Power law (degree distribution - social network) |

### Rich club, graph and motif

> Definition: **random walk** - A random walk (TODO: Add formal)


TODO: Add visualization (gif/js) for random walk power law vs text power law.

## Our method

Our method aims to improve the result of DeepWalk through context manipulation.
More conretely, instead of only perform random walk, we propose the concept of
biased walk for context generation.

> Definition: **biased walk** - A random walk on graph which choose the next
vertices based on a prior probability distribution.

The pseudocode for our algorithm:

```
M <- Defined motif walk
```
