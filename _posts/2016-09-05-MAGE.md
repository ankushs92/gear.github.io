---
layout: post
type: project
title: Motif-Aware Graph Embedding
subtitle: The benefit of guilding random walks by motifs
---

![It's like this]({{site.baseurl}}/img/mage_example.png)

As an electronics engineering major, I initially thought of "embedding" as a kind of small
sub-systems used as a small computer. Later, graph theory gives me
another view of "embedding":

> Graph embedding of a graph $$G$$ on a surface $$\Sigma$$ is a representation
of $$G$$ on $$\Sigma$$ in which points of $$\Sigma$$ are associated to vertices
and simple arcs are associated to edges such that connectivity is preserved and
there is no over-lapping between edges.

This view of "embedding" is more abtract, and somewhat more fun. I understand it
as the act of "drawing" a graph onto another surface (or space). It is well known
that any [planar graph](https://en.wikipedia.org/wiki/Planar_graph) can be embedded 
on a 2D surface, and any graph can be embedded on a 3D "surface". However, recently,
the word "embedding" seems to mean something more concrete to a machine learning practitioner:

> Embedding, as in "word embedding", refers to a set of modeling and feature learning
techniques that map structured tokens to vectors of real numbers.

I think embedding **is** dimensionality reduction or representation learning, 
just another different fancy name. Maybe, in the context of natural language processing
and graph processing, we call dimensionality reduction embedding (*＾∀ﾟ)b.

Recently, the field of graph embedding takes off by the adaptation of [Skipgram](https://)
model on graphs. The pioneer paper "
[DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652)" 
on this topic was published in KDD'14 by Peperozi et al. 
I [presented](https://gear.github.io/) this paper at a meet up at 
University of Tokyo on graph embedding in early 2016. In a nutshell, graph embedding _is_
word embedding. If we think of word and vertex as a same entity, the 
algorithm to learn a projection of a word token to some d-dimension real vector space can
be applied to a vertex in a graph to do the similar embedding. Thanks to some similarity 
between the frequency of verticies in random walks and words count in text, the authors 
of _Deepwalk_ has proposed to use random walks on a graph as a tool to generate the 
_graph context_. For me, I view such scheme as a duality. (I **love** duality)

> Definition: **random walk** - A random walk (TODO: Add formal)

| | Text | Graph |
| :--- | :--- | :--- |
| Token | Word | Vertex |
| Structure | Sentences | Connections |
| Distribution | Power law (frequency) | Power law (social networks) |

TODO: Add visualization (gif/js) for random walk power law vs text power law.

## Our method

Our method aims to improve the result of DeepWalk through context manipulation.
More conretely, instead of only perform random walk, we propose the concept of
biased walk for context generation.

> Definition: **biased walk** - A random walk on graph which choose the next
vertices based on a prior probability distribution.

The pseudocode for our algorithm:

```
M <- Defined motif walk
```
