---
layout: post
type: project
title: Motif-Aware Graph Embedding
subtitle: The benefit of guilding random walks by motifs
---

Recently, the field of graph embedding takes off by the adaptation of [Skipgram](https://)
model on graphs. The pioneer paper "[DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652)" on this topic was published in KDD'14 by Peperozi et al. 
I [presented](https://gear.github.io/) this paper at a meet up at 
University of Tokyo on graph embedding in early 2016. In a nutshell, graph embedding _is_
word embedding. If we think of word and vertex as a same entity, the 
algorithm to learn a projection of a word token to some d-dimension real vector space can
be applied to a vertex in a graph to do the similar embedding. Thanks to some similarity 
between the frequency of verticies in random walks and words count in text, the authors 
of _Deepwalk_ has proposed to use random walks on a graph as a tool to generate the 
_graph context_. For me, I view such scheme as a duality. (I **love** duality)

> Definition: **random walk** - A random walk (TODO: Add formal)

| | Text | Graph |
| :--- | :--- | :--- |
| Token | Word | Vertex |
| Structure | Sentences | Connections |
| Distribution | Power law (frequency) | Power law (social networks) |

TODO: Add visualization (gif/js) for random walk power law vs text power law.

## Our method

Our method aims to improve the result of DeepWalk through context manipulation.
More conretely, instead of only perform random walk, we propose the concept of
biased walk for context generation.

> Definition: **biased walk** - A random walk on graph which choose the next
vertices based on a prior probability distribution.

The pseudocode for our algorithm:

```
M <- Defined motif walk
```
