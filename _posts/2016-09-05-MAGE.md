---
layout: post
type: project
title: Motif-Aware Graph Embedding
subtitle: The benefit of guilding random walks by motifs
---

![It's like this]({{site.baseurl}}/img/mage_example.png)

As an electronics engineering major, I initially thought of "embedding" as a kind of small
sub-systems used as a small computer. Later, graph theory gives me
another view of "embedding":

> Graph embedding of a graph $$G$$ on a surface $$\Sigma$$ is a representation
of $$G$$ on $$\Sigma$$ in which points of $$\Sigma$$ are associated to vertices
and simple arcs are associated to edges such that connectivity is preserved and
there is no over-lapping between edges.

This view of "embedding" is more abtract, and somewhat more fun. I understand it
as the act of "drawing" a graph onto another surface (or space). It is well known
that any [planar graph](https://en.wikipedia.org/wiki/Planar_graph) can be embedded
on a 2D surface, and any graph can be embedded on a 3D "surface". However, recently,
the word "embedding" seems to mean something more concrete to a machine learning practitioner:

> Embedding, as in "word embedding", refers to a set of modeling and feature learning
techniques that map structured tokens to vectors of real numbers.

I think embedding **is** dimensionality reduction or representation learning,
just another different fancy name. Maybe, in the context of natural language processing
and graph processing, we call dimensionality reduction embedding :ok_hand:.

## Background

There were many [representation learning](https://arxiv.org/abs/1206.5538)
algorithms in the field of machine learning. One of the most famous method is
[Spectral Clustering](http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf)
in which the _similarity_ of data is preserved while the dimensionality is reduced.
More recently, the [t-SNE](https://lvdmaaten.github.io/tsne/) algorithm is widely used
for data visualization. The general objective of t-SNE is somewhat similar to
Spectral Clustering. However, instead of dealing with large matrix computation,
the authors solved the embedding problem by minimizing a loss function defined
for the representations of each data point in two spaces (original data space and
the embedding space). The result of t-SNE is a 2D or 3D representation of
high dimensional data.

Similar to the t-SNE technique, [DeepWalk](https://arxiv.org/abs/1403.6652) was
proposed by Perozzi et al at KDD'14. DeepWalk and other algorithms inspired by
it were based on the [Skipgram](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) model proposed by Mikolov et al in 2013.
By observing the similarity between the vertex frequencie distribution on random walks
in a graph and the word frequency distribution in text, Perozzi et al proposed
that we can use random walks on a graph to generate "sentences", and then learn
the node representation using the Skipgram model. The slide
for my talk introducing DeepWalk at The University of Tokyo can be found [here](https://cdn.rawgit.com/gear/Presentations/master/2016-deepwalk-Todai/DeepWalk_2016_UTokyo.pdf).

As mentioned above, _Deepwalk_ is an algorithm that uses the word2vec
software package on a generated artificial text. The "text" here is 
generated by performing random walks on graphs. Such idea is simple
and effective.

![Power law]({{site.url}}/img/deepwalk_dist.png)
_The power law is observed in both domains. This observation can be the motivation for Deepwalk._

Denote a random walk starting from vertex $$i$$ is $$W(v_i)$$. The set containing
all random walks is denoted $$\mathbf{C} = \cup_{i \in V} W(v_i)$$ with $$V$$ is the vertex set.
Under this notations, the optimization problem is formulated in the same way as the skipgram model:

$$\text{min}_{\Phi} = - \log P({v_{i-w}...v_{i+w}} | \Phi(v_i))$$

In the optimization problem above, $$\Phi(v_i)$$ is the mapping from a vertex to its vector
representation. The probability is assumed to be able to factor as:

$$P({v_{i-w}...v_{i+w}} | \Phi(v_i)) = \Pi_{j \in \text{window}(i)} P(v_j | \Phi(v_i))$$

From here there are two things to think about:
1. How to construct the formula for $$P(v_j | \Phi(v_i))$$
2. Deciding on the feasible normalization method for $$(v_j | \Phi(v_i))$$

For the first problem, two embedding matrices are introduced. The first matrix
is $$\Phi_C$$ - the context matrix storing the embedding of the centered vertices ($$v_i$$). 
The second matrix is $$\Phi_E$$ - the embedding matrix storing surround vertices' embedding vectors.
These two matrices are jointly-learned as the algorithm scan the corpus.

There are two popular solutions for the second problem: hierachial softmax and negative sampling (my personal favorite learning algorithm).
Hierachial softmax

![Deepwalk]({{site.url}}/img/deewalk_example.png)
_Demonstration of how Deepwalk works._

## Our method

We can make analogues for almost everything, and therefore analogues are... useless
(as said by RPF in his book "Surely You're Joking, Mr. Feynman"). However,
a little similarity between English text and a complex network may make it easier
to think and find a generalized solution to our problems:

| | **Text** | **Graph** |
| :--- | :--- | :--- |
| _Element_ | Word | Vertex |
| _(Semi) Structure_ | Sentences, grammar | Connections, subgraphs (motifs) |
| _Distribution_ | Power law (frequency) | Power law (degree) |


Our method aims to improve the result of DeepWalk through context manipulation.
More conretely, instead of only perform random walk, we propose the concept of
biased walk for context generation.

> Definition: **biased walk** - A random walk in which the next vertices is chosen based on a pre-defined probability distribution.
