---
layout: post
type: math
title: Principal Component Analysis
subtitle: The singular value decomposition of data matrix
---

The Principal Component Analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) is a technique used in many fields: data science, signal processing, mechanics, etc. As a student of machine learning, I should take sometime to at least review this technique; and maybe [ICA](https://en.wikipedia.org/wiki/Independent_component_analysis) too, in some future posts. 

---

## Reducing dimensionality

With a data science mind set, the key idea of PCA is to reduce the dimensionality of the data while retaining as much variation as possible in the result. Personally, I think of PCA as projecting the "cloud" of data points to a "flat" surface. More technically, PCA is particularly useful when we have a large number of variables. In such situation, we might want to look at the data from a point of view where one direction capture the most variance (the data spread out the most). A picture from [setosa](http://setosa.io/ev/principal-component-analysis/) illustrates this idea:   

![PCA 2D]({{site.baseurl}}/img/pca_2d.png)
_Under the transformation, our data now have large variance on pc1 and small variance on pc2. The data now can be represented only on pc1 without much information loss._

Let $$\mathbf{x}$$ be a vector containing $$p$$ random variables, we define the principal components of $$\mathbf{x}$$ as follow:

1. Find $$\alpha_1 \in R^p$$ such that:

    $$
    \left\{ 
    \begin{array}{l}
    \left\lVert\alpha_1\right\rVert_2 = 1 \\ 
    z_1 = \alpha_1 \mathbf{x} = \sum_{j=1}^p \alpha_{1j} \mathbf{x}_j \text{ has the largest variance.} %_
    \end{array}
    \right. 
    $$ 

2. Next, find $$\alpha_2 \in R^p$$ such that:

    $$
    \left\{ 
    \begin{array}{l}
    \left\lVert\alpha_2\right\rVert_2 = 1 \\ 
    z_2 = \alpha_2 \mathbf{x} = \sum_{j=1}^p \alpha_{2j} \mathbf{x}_j \text{ has the largest variance, } z_2 \text{ is uncorrelated with } z_1 %_
    \end{array}
    \right. 
    $$ 

To be clear, the algorithm in this paper (FLID) learns the parameters for the matrix $$W$$, 
which encodes each item onto a latent space $$d$$. In the latent space $$d$$, similar items
are considered to be similar clustered together and a subset of similar items will receive a low
probability to be selected, while a subset of diverse items will be more likely to be selected.

## Notations

| Notation | Explaination |
| :------- | :----------- |
| $$V$$ | Set of all items. |
| $$S$$ | Sub set of $$V$$. $$S \subseteq V$$ or $$S \in 2^V$$. |
| $$2^V$$ | Power set of $$V$$, this set contains all subsets of $$V$$. |
| $$F$$ | $$F: 2^V \rightarrow \mathcal{R}$$ is a **submodular** function returning a real value for a subset $$S$$. This function can be think of as a coverage function. |
| $$P(S)$$ | The probability that subset $$S$$ is selected as a representative (diverse) set. |
| $$\exp{(F(S))}$$ | A log-submodular model defined on $$F$$. $$P(S) \propto \exp{(F(S))}$$. | 
| $$u$$ | A vector of real scalars. Each element of this vector describe the quality of an item. | 
| $$W$$ | Feature matrix. This matrix is the variable of the learning framework. |
| $$w_i$$ | Row $$i$$ of matrix $$W$$. Item $$i$$ is described by this vector. |
| $$w_{i,d}$$ | Element $$d$$-th of row $$i$$ in the matrix $$W$$. |
| $$\mathcal{A}$$ | $$\mathcal{A} = \{(S,Y_S)\}$$ where $$S \subseteq V$$ and $$Y_S \in {0,1}$$ is the set of labeled data. $$S$$ represent subset of items and $$Y_S$$ represent if the item is generated by noise for NCE estimator. (I know this is fuzzy, more on this [later](http://gear.github.io/)) | 
| $$P_d(S)$$ | True underlying distribution for the subset $$S$$. |
| $$P_n(S)$$ | Generated noise distribution for the subset $$S$$. |
| $$\theta$$ | This symbol represent the parameters to be learnt in a machine learning model. In this situation, $$\theta$$ includes the feature matrix $$W$$ and the normalization factor for $$P_d(S)$$. |
| $$\nabla_u$$ | The gradient (of the likelihood) with respect to the quality vector $$u$$. |
| $$\nabla_W$$ | The gradient (of the likelihood) with respect to the feature matrix $$W$$. |
| $$\mathcal{D}$$ | The set of data samples. In this case, each samples is one subset $$S$$ of $$V$$. |
| $$\mathcal{N}$$ | The set of noise samples generated in NCE scheme. |
| $$\mathcal{T}$$ | The original test set for Amazon Baby Registries data. |
| $$\mathcal{T}'$$ | The new dataset created from $$\mathcal{T}$$ by removing a single elements from $$\mathcal{T}$$. $$\mathcal{T}' = \{(S \setminus \{i\}): S \in \mathcal{T}, size(S) > 2, i \in S \}$$ |
| $$\check{S}_i$$ | The set $$S$$ that has item $$i$$ removed. |
| $$\mathcal{L}$$ | Log-likelihood. |
| $$\mathcal{G}$$ | Item group. |
| $$\lambda$$ | A hyper-parameter for the truncated geometric distribution to penaltize large subsets. (This model prefers smaller subsets _obviously_).
| LLRI | Log-likelihood relative improvement compared to a naive modular model. |
| MMR | Mean reciprocal rank. A score which is larger if the model predict the ranking of an item correctly. |

## Model description

$$\begin{align} P(&S | \boldsymbol{u}, \boldsymbol{W}) \\ &=\frac{1}{Z}\exp{(\sum_{i\in S}u_i + \sum_{d=1}^{L}(\max_{i \in S} w_{i,d}))}\end{align}$$

The model above defines a distribution on subset $$S$$ and parameterized
by a quality vector $$\boldsymbol{u}$$ and (latent) feature matrix 
$$\boldsymbol{W}$$. In this formula, $$Z$$ is the normalization factor
and the exponent part is the unormalized model.

> It is difficult to define an already-normalized model, and maybe
we think mostly in term of unormalized model. Let's say you got admitted
to Hogwarts. When a teacher there thinks you did something good, they
award you with a score: "10 points for Slytherin!", and you are happy
with your 10 points. But... what does 10 points mean? This point scheme
seems really intuitive to me. 


![Accrual Structure]({{site.baseurl}}/img/phifail_istructure.png){:width="100%"}

![iCDFt]({{site.baseurl}}/img/phifail_plater.png){:width="50%" .center-small} 

![Flow]({{site.baseurl}}/img/phifail_flow.png)

## Determinantal Point Process

More detail is provided in the authors' paper.

## Evaluation metrics

## Datasets

## Results

## Some of my thought on the paper

The more I understand this model, the more _word2vec_ alike this FLID 
model seems to me. Given a set of items which belong to groups, learn
a feature vector for each item, and then use those vectors for some 
automated task. In this paper, the task is to select the most diverse
subset. On another perspective, I think this model is also similar to
Markov Random Field with repulsive (negative) potential.
