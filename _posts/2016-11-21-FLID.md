---
layout: post
type: math
title: Learning Probabilistic Submodular Diversity Models Via NCE
subtitle: A fast diversity selection model
---

This post is the summary of the paper: _**Learning Probabilistic Submodular Diversity
Models Via Noise Contrastive Estimation**_ by Sebastian Tschiatschek, Josip Djolonga,
and Andreas Krause. In International Conference on Artificial Intelligence and Statistic
(AISTATS'16). JMLR: W&CP volume 51. 

---

This research comes from the [Learning and Adaptive System](https://las.inf.ethz.ch/)
group of professor Andreas Krause. The group's main topic is submodularity function
and probabilistic modeling for machine learning applications. I find these topic
particulaly interesting, especially the research papers from this group have an
overall top quality.

## Problem definition

This paper addressed the problem of diversity selection from a set. Given a set
of _something_, we want to select a subset of the given set that maximizes
some requirement. For example, given a set of all research papers and some descriptions
about the content (topic) of the papers, how to select the set of 100 papers represent 
(or cover) the knowledge in the given set? One approach is to define a selection procedure
(a [_point process_](https://terrytao.wordpress.com/2009/08/23/determinantal-processes/)) 
in which the subset with the most coverage has a high probability of being selected. 

![Set of papers and feature vectors]({{site.baseurl}}/img/flid_example1.png)

_We have a set of scientific papers, each paper is associated with a feature vector describing
how relevant the paper is to a certain topic._

- **Input**: A set of items $$V$$ divided into some subsets of similarity, vector of scalar "qualities" $$\boldsymbol{u} = [u_i]_{i\in V}$$, and a scalar $$d$$ specify the dimensionality of the feature vectors of each item in the set $$V$$.
- **Output**: A matrix $$W$$ of size $$n \times d$$, where n is the number of item in $$V$$. Using this feature vector, we can define a log-submodular probabilistic model on subsets of $$V$$ which favors diversity.
- **Assumption**: Since the proposed algorithm in this paper uses noise contrastive estimation, it shares the same assumption of NCE. The first assumption is the true data distribution is the model. The second assumption is the convergence of NCE estimator is guaranteed with some probability in the limit of infinite data.

To be clear, the algorithm in this paper (FLID) learns the parameters for the matrix $$W$$, 
which encodes each item onto a latent space $$d$$. In the latent space $$d$$, similar items
are considered to be similar clustered together and a subset of similar items will receive a low
probability to be selected, while a subset of diverse items will be more likely to be selected.

## Notations

| Notation | Explaination |
| :------- | :----------- |
| $$V$$ | Set of all items. |
| $$S$$ | Sub set of $$V$$. $$S \subseteq V$$ or $$S \in 2^V$$. |
| $$2^V$$ | Power set of $$V$$, this set contains all subsets of $$V$$. |
| $$F$$ | $$F: 2^V \rightarrow \mathcal{R}$$ is a **submodular** function returning a real value for a subset $$S$$. This function can be think of as a coverage function. |
| $$P(S)$$ | The probability that subset $$S$$ is selected as a representative (diverse) set. |
| $$\exp{(F(S))}$$ | A log-submodular model defined on $$F$$. $$P(S) \propto \exp{(F(S))}$$. | 
| $$u$$ | A vector of real scalars. Each element of this vector describe the quality of an item. | 
| $$W$$ | Feature matrix. This matrix is the variable of the learning framework. |
| $$w_i$$ | Row $$i$$ of matrix $$W$$. Item $$i$$ is described by this vector. |
| $$w_{i,d}$$ | Element $$d$$-th of row $$i$$ in the matrix $$W$$. |
| $$\mathcal{A}$$ | $$\mathcal{A} = \{(S,Y_S)\}$$ where $$S \subseteq V$$ and $$Y_S \in {0,1}$$ is the set of labeled data. $$S$$ represent subset of items and $$Y_S$$ represent if the item is generated by noise for NCE estimator. (I know this is fuzzy, more on this [later](http://gear.github.io/)) | 
| $$P_d(S)$$ | True underlying distribution for the subset $$S$$. |
| $$P_n(S)$$ | Generated noise distribution for the subset $$S$$. |
| $$\theta$$ | This symbol represent the parameters to be learnt in a machine learning model. In this situation, $$\theta$$ includes the feature matrix $$W$$ and the normalization factor for $$P_d(S)$$. |
| $$\nabla_u$$ | The gradient (of the likelihood) with respect to the quality vector $$u$$. |
| $$\nabla_W$$ | The gradient (of the likelihood) with respect to the feature matrix $$W$$. |
| $$\mathcal{D}$$ | The set of data samples. In this case, each samples is one subset $$S$$ of $$V$$. |
| $$\mathcal{N}$$ | The set of noise samples generated in NCE scheme. |
| $$\mathcal{T}$$ | The original test set for Amazon Baby Registries data. |
| $$\mathcal{T}'$$ | The new dataset created from $$\mathcal{T}$$ by removing a single elements from $$\mathcal{T}$$. $$\mathcal{T}' = \{(S \setminus \{i\}): S \in \mathcal{T}, size(S) > 2, i \in S \}$$ |
| $$\check{S}_i$$ | The set $$S$$ that has item $$i$$ removed. |
| $$\mathcal{L}$$ | Log-likelihood. |
| $$\mathcal{G}$$ | Item group. |
| $$\lambda$$ | A hyper-parameter for the truncated geometric distribution to penaltize large subsets. (This model prefers smaller subsets _obviously_).
| LLRI | Log-likelihood relative improvement compared to a naive modular model. |
| MMR | Mean reciprocal rank. A score which is larger if the model predict the ranking of an item correctly. |

## Model description

$$\begin{align} P(&S | \boldsymbol{u}, \boldsymbol{W}) \\ &=\frac{1}{Z}\exp{(\sum_{i\in S}u_i + \sum_{d=1}^{L}(\max_{i \in S} w_{i,d}))}\end{align}$$

The model above defines a distribution on subset $$S$$ and parameterized
by a quality vector $$\boldsymbol{u}$$ and (latent) feature matrix 
$$\boldsymbol{W}$$. In this formula, $$Z$$ is the normalization factor
and the exponent part is the unormalized model.

> It is difficult to define an already-normalized model, and maybe
we think mostly in term of unormalized model. Let's say you got admitted
to Hogwarts. When a teacher there thinks you did something good, they
award you with a score: "10 points for Slytherin!", and you are happy
with your 10 points. But... what does 10 points mean? This point scheme
seems really intuitive to me. 


![Accrual Structure]({{site.baseurl}}/img/phifail_istructure.png){:width="100%"}

![iCDFt]({{site.baseurl}}/img/phifail_plater.png){:width="50%" .center-small} 

![Flow]({{site.baseurl}}/img/phifail_flow.png)

## Determinantal Point Process

More detail is provided in the authors' paper.

## Evaluation metrics

## Datasets

## Results

## Some of my thought on the paper

The more I understand this model, the more _word2vec_ alike this FLID 
model seems to me. Given a set of items which belong to groups, learn
a feature vector for each item, and then use those vectors for some 
automated task. In this paper, the task is to select the most diverse
subset. On another perspective, I think this model is also similar to
Markov Random Field with repulsive (negative) potential.
