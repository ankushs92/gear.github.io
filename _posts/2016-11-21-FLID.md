---
layout: post
type: math
title: Learning Probabilistic Submodular Diversity Models Via NCE
subtitle: A fast diversity selection model
---

This post is the summary of the paper: _**Learning Probabilistic Submodular Diversity
Models Via Noise Contrastive Estimation**_ by Sebastian Tschiatschek, Josip Djolonga,
and Andreas Krause. In International Conference on Artificial Intelligence and Statistic
(AISTATS'16). JMLR: W&CP volume 51. 

---

This research comes from the [Learning and Adaptive System](https://las.inf.ethz.ch/)
group of professor Andreas Krause. The group's main topic is submodularity function
and probabilistic modeling for machine learning applications. I find these topic
particulaly interesting, especially the research papers from this group have an
overall top quality.

## Problem definition

This paper addressed the problem of diversity selection from a set. Given a set
of _something_, we want to select a subset of the given set that maximizes
some requirement. For example, given a set of all research papers and some descriptions
about the content (topic) of the papers, how to select the set of 100 papers represent 
(or cover) the knowledge in the given set? One approach is to define a selection procedure
(a [_point process_](https://terrytao.wordpress.com/2009/08/23/determinantal-processes/)) 
in which the subset with the most coverage has high probability of being selected. 

![Set of papers and feature vectors]({{site.baseurl}}/img/flid_example1.png)

_We have a set of scientific papers, each paper is associated with a feature vector describing
how relevant the paper is to a certain topic._

- **Input**: A set of items $$V$$ divided into some subsets of similarity, vector of scalar "qualities" $$\boldsymbol{u} = [u_i]_{i\in V}$$, and a scalar $$d$$ specify the dimensionality of the feature vectors of each item in the set $$V$$.
- **Output**: A matrix $$W$$ of size $$n \times d$$, where n is the number of item in $$V$$. Using this feature vector, we can define a log-submodular probabilistic model on subsets of $$V$$ which favors diversity.
- **Assumption**: Since the proposed algorithm in this paper uses noise contrastive estimation, it shares the same assumption of NCE. The first assumption is the true data distribution is the model. The second assumption is the convergence of NCE estimator is guaranteed with some probability in the limit of infinite data.

To be clear, the algorithm in this paper (FLID) learns the parameters for the matrix $$W$$, 
which encodes each item onto a latent space $$d$$. In the latent space $$d$$, similar items
are considered to be similar clustered together and a subset of similar items will receive a low
probability to be selected, while a subset of diverse items will be more likely to be selected.

## Notations

| Notation | Explaination |
| :------- | :----------- |
| $$V$$ | Set of all items. |
| $$S$$ | Sub set of $$V$$. $$S \subseteq V$$ or $$S \in 2^V$$. |
| $$2^V$$ | Power set of $$V$$, this set contains all subsets of $$V$$. |
| $$F$$ | $$F: 2^V \rightarrow \mathcal{R}$$ is a **submodular** function returning a real value for a subset $$S$$. This function can be think of as a coverage function. |
| $$P(S)$$ | The probability that subset $$S$$ is selected as a representative (diverse) set. |
| $$\exp{(F(S))}$$ | A log-submodular model defined on $$F$$. $$P(S) \propto \exp{(F(S))}. | 

## Model description

![Accrual Structure]({{site.baseurl}}/img/phifail_istructure.png){:width="100%"}

![iCDFt]({{site.baseurl}}/img/phifail_plater.png){:width="50%" .center-small} 

![Flow]({{site.baseurl}}/img/phifail_flow.png)

## Determinantal Point Process

More detail is provided in the authors' paper.

## Evaluation metrics

## Datasets

## Results
